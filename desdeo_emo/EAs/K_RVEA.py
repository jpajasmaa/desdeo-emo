from typing import Dict, Union

from desdeo_emo.EAs import RVEA
from desdeo_emo.population.Population import Population
from desdeo_emo.utilities.model_management import ikrvea_mm
# from desdeo_emo.selection.APD_Select import APD_Select
from desdeo_emo.selection.APD_Select_constraints import APD_Select
from desdeo_emo.selection.oAPD import Optimistic_APD_Select
from desdeo_emo.selection.robust_APD import robust_APD_Select
from desdeo_problem import MOProblem
from numpy.core.numeric import indices
from numpy.lib.arraysetops import unique
from pandas.core.frame import DataFrame
from desdeo_tools.scalarization.ASF import SimpleASF, ReferencePointASF
from numba import njit
import numpy as np
import pandas as pd
import copy
from desdeo_problem.surrogatemodels.SurrogateModels import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct,\
    WhiteKernel, RBF, Matern, ConstantKernel
from desdeo_problem import MOProblem

from desdeo_problem import ExperimentalProblem
import sys 
import numpy as np
import pandas as pd

from sklearn.gaussian_process.kernels import Matern
from pymoo.factory import get_problem, get_reference_directions
import copy
from desdeo_tools.scalarization.ASF import SimpleASF


#TODO: add @njit function here
def remove_duplicate(
    X: np.ndarray,
    archive_x: np.ndarray
    ):
    """identifiesthe duplicate rows for decision variables
    Args:
    X (np.ndarray): the current decision variables.
    archive_x (np.ndarray): The decision variables in the archive.
    Returns: 
    indicies (np.ndarray): the indicies of solutions that are NOT already in the archive.
    """

    all_variables = np.vstack((archive_x,X))
    all_variables_indicies = np.arange(len(all_variables))

    e,unique_variables_indicies = np.unique(all_variables, return_index=True, axis=0)

    repeated_all_variables = np.delete(all_variables_indicies, unique_variables_indicies)
    repeated_in_X = repeated_all_variables - len(archive_x)
    X_indicies = np.arange(len(X))
    X_uniqe_indicies = np.delete(X_indicies, repeated_in_X)

        
    return X_uniqe_indicies



# mostly coping ikrvea_mm but no interactive part no reference point
def krvea_mm(
    individuals: np.ndarray,
    objectives: np.ndarray,
    uncertainity: np.ndarray,
    problem: MOProblem,
    u: int) -> float:
    """ Selects the solutions that need to be reevaluated with the original functions.
    This model management is based on the following papaer: 
    'P. Aghaei Pour, T. Rodemann, J. Hakanen, and K. Miettinen, “Surrogate assisted interactive
     multiobjective optimization in energy system design of buildings,” 
     Optimization and Engineering, 2021.'
    Args:
        reference_front (np.ndarray): The reference front that the current front is being compared to.
        Should be an one-dimensional array.
        individuals (np.ndarray): Current individuals generated by using surrogate models
        objectives (np.ndarray): Current objectives  generated by using surrogate models
        uncertainity (np.ndarray): Current Uncertainty values generated by using surrogate models
        problem : the problem class
    Returns:
        float: the new problem object that has an updated archive.
    """     
    
    nd = remove_duplicate(individuals, problem.archive.drop(
            problem.objective_names, axis=1).to_numpy()) #removing duplicate solutions
    if len(nd) == 0:
        return problem
    else:
        non_duplicate_dv = individuals[nd]
        non_duplicate_obj = objectives[nd]
        non_duplicate_unc = uncertainity[nd]
        
    # Selecting solutions with lowest ASF values
    #ref_point = np.array([0.5, 0.5, 0.5]) # only for testing
    #asf_solutions = SimpleASF([1]*problem.n_of_objectives).__call__(non_duplicate_obj, ref_point)

    #solutions = non_duplicate_obj

    #idx = np.argpartition(asf_solutions, 2*u)
    #asf_unc = np.max(non_duplicate_unc [idx[0:2*u]], axis= 1)
    # index of solutions with lowest Uncertainty
    #lowest_unc_index = np.argpartition(asf_unc, u)[0:u]
    # evaluating the solutions in asf_unc with lowest uncertainty. The archive will get update in problem.evaluate()
    #problem.evaluate(non_duplicate_dv[lowest_unc_index], use_surrogate=False)[0]


    # offline 
    #problem.evaluate(non_duplicate_dv, use_surrogate=True)[0]
    
    # online, update all solutions
    problem.evaluate(non_duplicate_dv, use_surrogate=False)[0]

    # update the model   
    problem.train(models=GaussianProcessRegressor,\
         model_parameters={'kernel': Matern(nu=1.5)}) 

    return problem


class K_RVEA(RVEA):
    """The python version Interactive Kriging-assisted reference vector guieded evolutionary algorithm (IK-RVEA).
    Most of the relevant code is contained in the super class. This class just assigns
    the APD selection operator, and the model management to BaseDecompositionEA.
    NOTE: The APD (from RVEA) function had to be slightly modified to accomodate for the fact that
    this version of the algorithm is interactive, and does not have a set termination
    criteria. There is a time component in the APD penalty function formula of the type:
    (t/t_max)^alpha. As there is no set t_max, the formula has been changed. See below,
    the documentation for the argument: penalty_time_component
    See the details of IKRVEA in the following paper
    'P. Aghaei Pour, T. Rodemann, J. Hakanen, and K. Miettinen, “Surrogate assisted interactive
    multiobjective optimization in energy system design of buildings,” 
    Optimization and Engineering, 2021.'
    
    See the details of RVEA in the following paper
    R. Cheng, Y. Jin, M. Olhofer and B. Sendhoff, A Reference Vector Guided
    Evolutionary Algorithm for Many-objective Optimization, IEEE Transactions on
    Evolutionary Computation, 2016
    Parameters
    ----------
    problem : MOProblem
        The problem class object specifying the details of the problem.
    population_size : int, optional
        The desired population size, by default None, which sets up a default value
        of population size depending upon the dimensionaly of the problem.
    population_params : Dict, optional
        The parameters for the population class, by default None. See
        desdeo_emo.population.Population for more details.
    initial_population : Population, optional
        An initial population class, by default None. Use this if you want to set up
        a specific starting population, such as when the output of one EA is to be
        used as the input of another.
    alpha : float, optional
        The alpha parameter in the APD selection mechanism. Read paper for details.
    lattice_resolution : int, optional
        The number of divisions along individual axes in the objective space to be
        used while creating the reference vector lattice by the simplex lattice
        design. By default None
    selection_type : str, optional
        One of ["mean", "optimistic", "robust"]. To be used in data-driven optimization.
        To be used only with surrogate models which return an "uncertainity" factor.
        Using "mean" is equivalent to using the mean predicted values from the surrogate
        models and is the default case.
        Using "optimistic" results in using (mean - uncertainity) values from the
        the surrogate models as the predicted value (in case of minimization). It is
        (mean + uncertainity for maximization).
        Using "robust" is the opposite of using "optimistic".
    a_priori : bool, optional
        A bool variable defining whether a priori preference is to be used or not.
        By default False
    interact : bool, optional
        A bool variable defining whether interactive preference is to be used or
        not. By default False
    n_iterations : int, optional
        The total number of iterations to be run, by default 10. This is not a hard
        limit and is only used for an internal counter.
    n_gen_per_iter : int, optional
        The total number of generations in an iteration to be run, by default 100.
        This is not a hard limit and is only used for an internal counter.
    number_of_update: int, optional
        The number of solutions that are selected for true function evaluations, by default 10.
        This is not a hard limit and is set based on amount of time the user has and how long each true evaluation takes.
    total_function_evaluations :int, optional
        Set an upper limit to the total number of function evaluations. When set to
        zero, this argument is ignored and other termination criteria are used.
    penalty_time_component: Union[str, float], optional
        The APD formula had to be slightly changed.
        If penalty_time_component is a float between [0, 1], (t/t_max) is replaced by
        that constant for the entire algorithm.
        If penalty_time_component is "original", the original intent of the paper is
        followed and (t/t_max) is calculated as
        (current generation count/total number of generations).
        If penalty_time_component is "function_count", (t/t_max) is calculated as
        (current function evaluation count/total number of function evaluations)
        If penalty_time_component is "interactive", (t/t_max)  is calculated as
        (Current gen count within an iteration/Total gen count within an iteration).
        Hence, time penalty is always zero at the beginning of each iteration, and one
        at the end of each iteration.
        Note: If the penalty_time_component ever exceeds one, the value one is used as
        the penalty_time_component.
        If no value is provided, an appropriate default is selected.
        If `interact` is true, penalty_time_component is "interactive" by default.
        If `interact` is false, but `total_function_evaluations` is provided,
        penalty_time_component is "function_count" by default.
        If `interact` is false, but `total_function_evaluations` is not provided,
        penalty_time_component is "original" by default.
    """

    def __init__(
            self,
            problem: MOProblem,
            population_size: int = None,
            population_params: Dict = None,
            initial_population: Population = None,
            alpha: float = 2,
            lattice_resolution: int = None,
            selection_type: str = None,
            #a_priori: bool = False,
            interact: bool = False,
            use_surrogates: bool = False,
            n_iterations: int = 10,
            n_gen_per_iter: int = 100,
            number_of_update: int = 10,
            total_function_evaluations: int = 0,
            time_penalty_component: Union[str, float] = None,
    ):
        super().__init__(
            problem=problem,
            population_size=population_size,
            population_params=population_params,
            initial_population=initial_population,
            lattice_resolution=lattice_resolution,
            #a_priori=a_priori,
            interact=interact,
            use_surrogates=use_surrogates,
            n_iterations=n_iterations,
            n_gen_per_iter=n_gen_per_iter,
            total_function_evaluations=total_function_evaluations,
        )

        self.number_of_update = number_of_update # number of solutions that we use to update surrogates


    def iterate(self):
        super().iterate()
        updated_problem = krvea_mm(
        self.population.individuals,
        self.population.objectives,
        self.population.uncertainity,
        self.population.problem,
        self.number_of_update)
        self.population.problem = updated_problem

    def requests(self):
        return (self.request_preferences(), self.request_plot())

if __name__=="__main__":
    def warn(*args, **kwargs):
        pass
    import warnings
    warnings.warn = warn
    def obj_function1(x):

        out = {
        "F": "",
        "G": "",
        }
        problem = get_problem("dtlz2", 10)
        problem._evaluate(x, out)
        return out['F'][:,0]
    def obj_function2(x):

        out = {
        "F": "",
        "G": "",
        }
        problem = get_problem("dtlz2", 10)
        problem._evaluate(x, out)
        return out['F'][:,1]
    def obj_function3(x):

        out = {
        "F": "",
        "G": "",
        }
        problem = get_problem("dtlz2", 10)
        problem._evaluate(x, out)
        return out['F'][:,2]


    refpoint = np.asarray([0.2,0.5,0.9])
    n_obj = 3
    n_var = n_obj + 7
    var_names = ["x" + str(i + 1) for i in range(n_var)]
    obj_names = ["f" + str(i + 1) for i in range(n_obj)]
    unc_names = ["unc" + str(i + 1) for i in range(n_obj)]
# fundumentals of problem:

#creating the initial population
    x = np.random.random((109, n_var))
    initial_obj = {
        "F": "",
        "G": "",
        }
    get_problem("dtlz2", 12)._evaluate(x, initial_obj)

    data = np.hstack((x, initial_obj['F']))
    datapd = pd.DataFrame(data=data, columns=var_names+obj_names)

    problem = ExperimentalProblem(data = datapd, objective_names=obj_names, variable_names=var_names,\
         uncertainity_names=unc_names, evaluators = [obj_function1, obj_function2, obj_function3])
    problem.train(models=GaussianProcessRegressor, model_parameters={'kernel': Matern(nu=1.5)})
    u = 10 #number of solutions that we use to update surrogates in each iteration
    evolver = K_RVEA(
                problem, interact=False, n_iterations=1, n_gen_per_iter = 50,\
                     lattice_resolution=10, use_surrogates= True, selection_type="mean",  population_size= 109, total_function_evaluations=150, number_of_update=u)
    problem.train(models=GaussianProcessRegressor, model_parameters={'kernel': Matern(nu=1.5)})

    while evolver.continue_iteration():
        evolver.iterate()

    objectives = problem.archive.drop(problem.variable_names, axis=1).to_numpy()
    Best_solutions = objectives

    # we can ignore the interaction stuff
    #asf_values = SimpleASF([1]*problem.n_of_objectives).__call__(objectives, refpoint)
    #idx = np.argpartition(asf_values, u)[:u] #inddicies of best solutions based on ASF
    #Best_solutions = objectives[idx]

    import plotly.graph_objs as go
    from pymoo.factory import get_problem, get_reference_directions, get_visualization
    #the next three lines are to get the true pareto front
    p = get_problem("dtlz2", 10)
    ref_dirs = get_reference_directions("das-dennis", 3, n_partitions=10)
    pf = p.pareto_front(ref_dirs)


    x = Best_solutions[:,0]
    y = Best_solutions[:,1]
    z = Best_solutions[:,2]

    trace1 = go.Scatter3d(x=x, y=y, z=z, mode="markers",)
    trace2 = go.Scatter3d(x=[refpoint[0]], y=[refpoint[1]], z=[refpoint[2]], mode="markers")
    trace3 = go.Mesh3d(x=pf[:,0], y=pf[:,1], z=pf[:,2])
    fig = go.Figure(data = [trace1, trace2, trace3])
    fig.show()
